{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, argparse\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''get_feature from VGG16'''\n",
    "def get_image_features(image_file_name, CNN_weights_file_name):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the \n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    image_features = np.zeros((1, 4096))\n",
    "    # Magic_Number = 4096  > Comes from last layer of VGG Model\n",
    "\n",
    "    # Since VGG was trained as a image of 224x224, every new image\n",
    "    # is required to go through the same transformation\n",
    "    im = cv2.resize(cv2.imread(image_file_name), (224, 224))\n",
    "    im = im.transpose((2,0,1)) # convert the image to RGBA\n",
    "\n",
    "    \n",
    "    # this axis dimension is required because VGG was trained on a dimension\n",
    "    # of 1, 3, 224, 224 (first axis is for the batch size\n",
    "    # even though we are using only one image, we have to keep the dimensions consistent\n",
    "    im = np.expand_dims(im, axis=0) \n",
    "\n",
    "    image_features[0,:] = get_image_model(CNN_weights_file_name).predict(im)[0]\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''embedding'''\n",
    "def get_question_features(question):\n",
    "    ''' For a given question, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    word_embeddings = spacy.load('en', vectors='en_glove_cc_300_1m_vectors')\n",
    "    tokens = word_embeddings(question)\n",
    "    question_tensor = np.zeros((1, len(tokens), 300))\n",
    "    for j in xrange(len(tokens)):\n",
    "            question_tensor[0,j,:] = tokens[j].vector\n",
    "    return question_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''combine'''\n",
    "def get_VQA_model(VQA_model_file_name, VQA_weights_file_name):\n",
    "    ''' Given the VQA model and its weights, compiles and returns the model '''\n",
    "\n",
    "    # thanks the keras function for loading a model from JSON, this becomes\n",
    "    # very easy to understand and work. Alternative would be to load model\n",
    "    # from binary like cPickle but then model would be obfuscated to users\n",
    "    vqa_model = model_from_json(open(VQA_model_file_name).read())\n",
    "    vqa_model.load_weights(VQA_weights_file_name)\n",
    "    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''simulate'''\n",
    "image_file_name = 'test.jpg'\n",
    "question = u\"What vehicle is in the picture?\"\n",
    "\n",
    "# get the image features\n",
    "image_features = get_image_features(image_file_name, CNN_weights_file_name)\n",
    "\n",
    "y_output = model_vqa.predict([question_features, image_features])\n",
    "\n",
    "# This task here is represented as a classification into a 1000 top answers\n",
    "# this means some of the answers were not part of training and thus would \n",
    "# not show up in the result.\n",
    "# These 1000 answers are stored in the sklearn Encoder class\n",
    "labelencoder = joblib.load(label_encoder_file_name)\n",
    "for label in reversed(np.argsort(y_output)[0,-5:]):\n",
    "    print str(round(y_output[0,label]*100,2)).zfill(5), \"% \", labelencoder.inverse_transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
